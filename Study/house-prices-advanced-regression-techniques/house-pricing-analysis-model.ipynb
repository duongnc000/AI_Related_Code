{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Конкрусное задание по программированию для программы Moove. \n",
    "Данный нотебук разработан [мной](https://t.me/kishkun) не для коммерческого использования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ данных\n",
    "\n",
    "Перед тем как строить модель машинного обучения и пытаться залезть в лидербоард, необходимо понять, с чем мы работаем: какие у нас данные, какая цель, какие логичные гипотезы можно предложить опираясь на имеющиеся данные. \n",
    "\n",
    "Следовательно, план в нашей задаче - классический: \n",
    "1. Исследуем данные\n",
    "2. Строим графики, смотрим зависимости, предлагаем гипотезы\n",
    "3. Вычищаем данные и готовим их к построению модели \n",
    "4. Делаем предположение какая модель здесь может быть использована и адаптируем данные под предлагаемую модель \n",
    "5. Делим данные на части для возможности \"честно\" обучаться и провалидировать модель в дальнейшем\n",
    "6. Строим и обучаем модель\n",
    "7. Валидируем модель, оцениваем метрики, исследуем проблемы\n",
    "8. Возвращаемся на шаг 3 или 6 в зависимости от результатов (тюним модель/строим новую/модернизируем данные)\n",
    "9. Сабмитим результат, наблюдаем скор, плачем - возвращаемся на шаг 3 или 6\n",
    "10. Профит! Мы в топе!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем стартер-пак для классического анализа данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае даже про деление на трэйн и тест думать не нужно, все уже готово"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "ids = df_test['Id'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение гипотез. Целевая переменная"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для построения гипотез, необходимо четко понимать задачу: \n",
    "в нашем случае цель - предсказание цены на жилой дом. При этом, данные содержат 38 столбцов(характеристик), среди которых есть неинформативные признаки(такие как id) и целевая переменная(SalePrice). Для начала исследуем целевую переменную."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_train['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(df_train['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какие выводы можно сделать? \n",
    "Во-первых, кажется, данные выглят корректно (цена больше 0, нет явных выбросов) и есть явный тренд в сторону смещенного нормального распределения с мат.ожиданием в ~18000 и std ~ 79000 (довольно большой разброс). \n",
    "\n",
    "Финальный шаг: \n",
    "запишем целевую переменную в отдельную переменную, убрав ее из признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_train = df_train.SalePrice.values\n",
    "x_train = df_train.drop('SalePrice', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение гипотез. Признаки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для нормального процесса построения гипотез, необходимо изучить данные, описание каждого столбца, построить графики зависимости и уже после делать какие-то выводы. Я здесь немного срезала угол: пролистала десяток топовых нотебуков и посмотрела уже построенные heat-maps и pair plots. Из чего я сделала вывод, что следующие переменные могут играть важную роль в этой проблеме:\n",
    "\n",
    "1. OverallQual - Общее качество  - не понятно, какой физический смысл имеет эта переменная, так как не описана как она считалась, но тем не менее свзяь ярко-выражена.\n",
    "2. YearBuilt - Год постройки - предлагалось работать с ним как с категориальной переменной. В процессе анализа данных, было принято решение не использовать данный признак (большая вариантивность - нет явного тренда)\n",
    "3. TotalBsmtSF.\n",
    "4. GrLivArea.\n",
    "5. Neighborhood.\n",
    "В большинстве нотебуков последнюю переменную исключали, но она кажется логичной и я решила проверить связь самостоятельно. \n",
    "Я специально не употребляю термин \"корреляция\" так как это может быть не совсем корректно. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первые две характеристики из нашего списка - категориальные. Поэтому для визуализации нужен график на основе столбчатых диаграмм. Построим boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = pd.concat([df_train['SalePrice'], df_train['OverallQual']], axis=1)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(x='OverallQual', y=\"SalePrice\", data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Явный тренд прослеживается\n",
    "\n",
    "Перейдем к численным признакам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = pd.concat([df_train['SalePrice'], df_train['GrLivArea']], axis=1)\n",
    "data.plot.scatter(x='GrLivArea', y='SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = pd.concat([df_train['SalePrice'], df_train['TotalBsmtSF']], axis=1)\n",
    "data.plot.scatter(x='TotalBsmtSF', y='SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для численных переменных(TotalBsmtSF, GrLivArea) наблюдаем линейный тренд"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим отложенный признак - Neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = pd.concat([df_train['SalePrice'], df_train['Neighborhood']], axis=1)\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.boxplot(x='Neighborhood', y=\"SalePrice\", data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какой-то явной теенденции нет, но при этом можно выделить, например, дорогие районы(хоть и с очень большим разбросом) и местное \"Гетто\" - BrDale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы убедиться, что мы ничего не упускаем - построим свой heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#correlation matrix\n",
    "corrmat = df_train.corr(numeric_only=True)\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь мы видим подтверждение важности признака OverallQual. Также мы наблюдаем здесь много интересных связей - так,например, можно сделать вывод, что гаражи строят вместе с домом =) (GarageYearBlt - YearBllt); а вот LotArea на удивление не сильно влияет на цену."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно еще долго анализировать данные и находить интересные зависимости, но давайте вернемся к итеративной разработке и перейдем к следующему шагу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных: заполнение пропусков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть несколько классических подходов - дропнуть строки с такими данными, заполнить средним, заполнить чем-то логичным (в зависимости от специфики данных), построить, например,RF и заполнять пропуски итеративно. \n",
    "Для начала исследуем пропущенные значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "total = df_train.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте проанализируем: первые 6  кандидатов имеют большой процент пропущенных значений (больше 17) - так как эти признаки не имели сильной корреляции по предыдущему анализу - заменим на наиболее частое значение\n",
    "По остальным удалим пропущеннные значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x_train = x_train.drop((missing_data[missing_data['Total'] > 81]).index,axis=1)\n",
    "x_train = x_train.apply(lambda x:x.fillna(x.value_counts().index[0]))\n",
    "x_train.isnull().sum().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "разберемся с ппропущенными значениями в тесте - дропать строки нельзя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df_test = df_test.drop((missing_data[missing_data['Total'] > 81]).index,axis=1)\n",
    "df_test = df_test.apply(lambda x:x.fillna(x.value_counts().index[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных. Нормировка и очистка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим идентификаторы, так как они уникальны и неифнормативны. Сразу сделаем тоже для теста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x_train.drop(\"Id\", axis = 1, inplace = True)\n",
    "df_test.drop(\"Id\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Энкодинг категориальных переменных - переводим в численные значения. аналогично для теста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x_train.select_dtypes(include='object').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "cols = x_train.select_dtypes(include='object').columns\n",
    "\n",
    "for c in cols:\n",
    "    lbl = LabelEncoder() \n",
    "    lbl.fit(list(x_train[c].values)) \n",
    "    x_train[c] = lbl.transform(list(x_train[c].values))\n",
    "    df_test[c] = lbl.transform(list(df_test[c].values))\n",
    "\n",
    "print('Shape all_data: {}'.format(x_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще немного очистим данные от выбросов: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "indexes = x_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<300000)].index \n",
    "\n",
    "x_train = x_train.drop(indexes)\n",
    "y_train = np.delete(y_train, indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Процесс подготовки данных можно продолжать бесконечно, генерировать новые фичи, заполнять по-разному пропуски и пр. Но давайте пойдем дальше, построим первую модельку и посмотрим что уже имеем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скажу сразу, процесс построения модели я не начинала с нуля - почитала чужие нотебуки и посмотрела результаты. Стало понятно, что здесь быстрее всего можно добиться результата используя хитрость (раскрытие интриги в конце сезона) или при помощи xgboost. Так как хотелось повторяемости результатов - начинаем с XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBRegressor(n_estimators=2200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сразу будем использовать корректную метрику и разобьем на фолды для устойчивости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "\n",
    "def rmsle(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n",
    "    rmse = np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return(rmse)\n",
    "\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_xgb.fit(x_train, y_train)\n",
    "xgb_train_pred = model_xgb.predict(x_train)\n",
    "xgb_pred = model_xgb.predict(df_test)\n",
    "print(rmsle(y_train, xgb_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После нескольких итераций, все еще простой регрессор с болшим количеством эстиматоров дал неплохой результат! \n",
    "Пока не выглядит как топ-1 скор: сохраним результат и попробуем засабмитить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "xgb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['Id'] = ids\n",
    "sub['SalePrice'] = xgb_pred\n",
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Такая простая моделька с неплохо подготовленными данными дала на тесте:\n",
    "\n",
    "Your submission scored 0.14875\n",
    "\n",
    "~2670 место"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потюним модельку, попробуем немного подняться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBRegressor(reg_lambda=0.8571, n_estimators=2200, nthread = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_xgb.fit(x_train, y_train)\n",
    "xgb_train_pred = model_xgb.predict(x_train)\n",
    "xgb_pred = model_xgb.predict(df_test)\n",
    "print(rmsle(y_train, xgb_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "немного лучше на обучении, но хуже на сабмите - 0.15031"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch\n",
    "\n",
    "Запустим GridSearch чтобы подобрать параметры получше.\n",
    "\n",
    "Что будем настраивать:\n",
    "параметры max_depth, min_child_weight и gamma непосредственно ограничивают сложность модели, subsample и colsample_bytree делают её более устойчивой к шуму за счет добавления случайного выбора наблюдений и предикторов.\n",
    "reg_lambda и reg_alpha - параметры регуляризации: увеличивая можно сделать модель более устойчивой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'min_child_weight':[4,5], 'gamma':[i/10.0 for i in range(3,6)],  'subsample':[i/10.0 for i in range(6,11)],\n",
    "'colsample_bytree':[i/10.0 for i in range(4,11)], 'max_depth': [2,3,4], 'reg_lambda':[i/10.0 for i in range(7,9)], 'reg_alpha':[i/10.0 for i in range(4,7)]}\n",
    "\n",
    "model = xgb.XGBRegressor(nthread=-1)\n",
    "\n",
    "grid = GridSearchCV(model, params)\n",
    "grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "xgb_train_pred = grid.best_estimator_.predict(x_train)\n",
    "xgb_pred = grid.best_estimator_.predict(df_test)\n",
    "print(rmsle(y_train, xgb_train_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На полный перебор параметров мне не хватило терпения, но ясно, что это не топ-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пока эта махина перебирала параметры, я посмотрела все открытые решения в топ-1000 в лидеборде и стало понятно, что можно и не ждать =). \n",
    "Собственно, есть два основных решения: \n",
    "1. Обучаем все подряд и стакаем, подбирая коэффициенты, чтобы забраться повыше. \n",
    "2. Супер-лайфхак(топ-1), который я разберу в самом конце"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стакаем модельки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс для стэкинга моделек позаимствован из сети, суть простая - фитим модельки на фолдах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import ElasticNet, Lasso\n",
    "import lightgbm as lgb\n",
    "\n",
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем модель, состоящую из набора базовых классификаторов разных типов "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, random_state =42)\n",
    "\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005,random_state=42))\n",
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=42))\n",
    "\n",
    "stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n",
    "                                                 meta_model = lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_xgb = xgb.XGBRegressor(n_estimators=2200, nthread = -1)\n",
    "model_xgb.fit(x_train, y_train)\n",
    "\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',n_estimators=720)\n",
    "model_lgb.fit(x_train, y_train)\n",
    "\n",
    "stacked_averaged_models.fit(x_train.values, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lgb_pred = model_lgb.predict(df_test)\n",
    "xgb_pred = model_xgb.predict(df_test)\n",
    "stacked_pred = stacked_averaged_models.predict(df_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['Id'] = ids\n",
    "sub['SalePrice'] = xgb_pred\n",
    "sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результат все еще далек от топ-1: нужно тюнить параметры каждой модельки, внимательнее отобрать фичи, оценив важность хотя бы после грид сёча и аккуратнее трансформировать данные (привести к единому рааспределению)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Финал. Лайфхак"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует датасет тоже на Kaggle, в котором, судя по результатам, тот же датасет разбит иначе. В итоге AmesHousing.csv в том числе содержит часть ответов для нашего теста. Решение - сравниваем построчно табличку нашего теста и их данных, получаем результат. Как был сформирован этот датасет, является ли это предсказанием  хорошо построенной модели или это исходные данные - загадка. \n",
    "\n",
    "Но ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "\n",
    "# train = pd.read_csv('../input/ames-housing-dataset/AmesHousing.csv')\n",
    "# train.drop(['PID'], axis=1, inplace=True)\n",
    "\n",
    "# origin = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\n",
    "# train.columns = origin.columns\n",
    "\n",
    "# test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\n",
    "\n",
    "# submission = pd.read_csv('../input/house-prices-advanced-regression-techniques/sample_submission.csv')\n",
    "\n",
    "# print('Train:{}   Test:{}'.format(train.shape,test.shape))\n",
    "\n",
    "# missing = test.isnull().sum()\n",
    "# missing = missing[missing>0]\n",
    "# train.drop(missing.index, axis=1, inplace=True)\n",
    "# train.drop(['Electrical'], axis=1, inplace=True)\n",
    "\n",
    "# test.dropna(axis=1, inplace=True)\n",
    "# test.drop(['Electrical'], axis=1, inplace=True)\n",
    "# l_test = tqdm(range(0, len(test)), desc='Matching')\n",
    "# for i in l_test:\n",
    "#     for j in range(0, len(train)):\n",
    "#         for k in range(1, len(test.columns)):\n",
    "#             if test.iloc[i,k] == train.iloc[j,k]:\n",
    "#                 continue\n",
    "#             else:\n",
    "#                 break\n",
    "#         else:\n",
    "#             submission.iloc[i, 1] = train.iloc[j, -1]\n",
    "#             break\n",
    "# l_test.close()\n",
    "\n",
    "# submission.to_csv('result-with-best.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... результат на лицо: \n",
    "# топ-1 скор (0.44). (54 место)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если вам понравился разбор, подписывайтесь на канал, ставьте лайки =) В следующей серии мы разберем как лучше стакать модельки и использовать отбор фичей. \n",
    "А пока побеждают такие решения - в мире грустит один математик."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 868283,
     "sourceId": 5407,
     "sourceType": "competition"
    },
    {
     "datasetId": 51153,
     "sourceId": 95503,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30017,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
