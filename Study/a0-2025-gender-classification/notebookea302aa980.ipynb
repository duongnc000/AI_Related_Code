{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-01T17:20:00.743169Z",
     "iopub.status.busy": "2025-04-01T17:20:00.742693Z",
     "iopub.status.idle": "2025-04-01T17:20:00.748287Z",
     "shell.execute_reply": "2025-04-01T17:20:00.747254Z",
     "shell.execute_reply.started": "2025-04-01T17:20:00.743129Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm  # Import tqdm for progress bars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T17:20:12.940099Z",
     "iopub.status.busy": "2025-04-01T17:20:12.939638Z",
     "iopub.status.idle": "2025-04-01T17:20:12.952325Z",
     "shell.execute_reply": "2025-04-01T17:20:12.951244Z",
     "shell.execute_reply.started": "2025-04-01T17:20:12.940054Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T17:33:11.766470Z",
     "iopub.status.busy": "2025-04-01T17:33:11.766007Z",
     "iopub.status.idle": "2025-04-01T17:33:18.842480Z",
     "shell.execute_reply": "2025-04-01T17:33:18.841319Z",
     "shell.execute_reply.started": "2025-04-01T17:33:11.766434Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "transform_random = transforms.Compose([\n",
    "    transforms.RandomRotation(10),  # Rotate images by up to 10 degrees\n",
    "    transforms.RandomResizedCrop(128, scale=(0.9, 1.1)),  # Zoom augmentation\n",
    "    transforms.RandomAffine(0, translate=(0.1, 0.1)),  # Width and height shift\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Load Dataset\n",
    "# train_dataset = torchvision.datasets.ImageFolder(root=\"Dataset/Train\", transform=transform)\n",
    "# val_dataset = torchvision.datasets.ImageFolder(root=\"Dataset/Val\", transform=transform)\n",
    "\n",
    "# Create DataLoaders with batching\n",
    "# batch_size = 32\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T17:34:09.503874Z",
     "iopub.status.busy": "2025-04-01T17:34:09.503413Z",
     "iopub.status.idle": "2025-04-01T17:34:09.511057Z",
     "shell.execute_reply": "2025-04-01T17:34:09.509823Z",
     "shell.execute_reply.started": "2025-04-01T17:34:09.503834Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define CNN Model\n",
    "class GenderClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GenderClassifier, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(128 * 16 * 16, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T17:34:43.520073Z",
     "iopub.status.busy": "2025-04-01T17:34:43.519607Z",
     "iopub.status.idle": "2025-04-01T17:34:43.699028Z",
     "shell.execute_reply": "2025-04-01T17:34:43.697870Z",
     "shell.execute_reply.started": "2025-04-01T17:34:43.520038Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize Model, Loss, and Optimizer\n",
    "model = GenderClassifier().to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.95 ** epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets = 15\n",
    "models = [GenderClassifier().to(device) for _ in range(nets)]\n",
    "criterion = nn.BCELoss()\n",
    "optimizers = [optim.Adam(model.parameters(), lr=0.001) for model in models]\n",
    "schedulers = [optim.lr_scheduler.LambdaLR(optimizer, lambda epoch: 0.95 ** epoch) for optimizer in optimizers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T17:34:58.395106Z",
     "iopub.status.busy": "2025-04-01T17:34:58.394647Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train Model with tqdm Progress Bar\n",
    "num_epochs = 25\n",
    "\n",
    "for j in range(nets):\n",
    "    model = models[j]\n",
    "    optimizer = optimizers[j]\n",
    "    scheduler = schedulers[j]\n",
    "\n",
    "    # Load Dataset\n",
    "    train_dataset = torchvision.datasets.ImageFolder(root=\"Dataset/Train\", transform=transform_random)\n",
    "    # Create DataLoaders with batching\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=True)\n",
    "\n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=running_loss / len(train_loader))  # Update tqdm bar\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"CNN {j + 1}: Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = torchvision.datasets.ImageFolder(root=\"Dataset/Val\", transform=transform)\n",
    "val_loader = DataLoader(val_dataset)\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device).float().unsqueeze(1)\n",
    "        outputs = 0\n",
    "        for j in range(nets):\n",
    "            outputs += model(images)\n",
    "        predicted = (outputs > nets / 2).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Validation Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "img = Image.open(\"Dataset/Test/000067.jpg\").convert(\"RGB\")\n",
    "# plt.imshow(cv2.cvtColor(cv2.imread(\"Dataset/Test/000067.jpg\"), cv2.COLOR_BGR2RGB))\n",
    "# plt.show()\n",
    "# img.show()\n",
    "img = transform(img).unsqueeze(0).to(device) \n",
    "output = model(img)\n",
    "print(output.item())  # Output is a tensor, convert to float for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = \"Dataset/Test\"\n",
    "\n",
    "# Load images\n",
    "test_images = []\n",
    "img_id = []\n",
    "for img_name in os.listdir(test_path):\n",
    "    img_path = os.path.join(test_path, img_name)\n",
    "    if img_name.endswith(('.jpg', '.png', '.jpeg')):  # Filter image files\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img = transform(img)\n",
    "        test_images.append(img)\n",
    "        # remove file extension for ID\n",
    "        img_id.append(os.path.splitext(img_name)[0])\n",
    "\n",
    "# Convert to tensor batch\n",
    "test_images = torch.stack(test_images)\n",
    "print(test_images.shape)  # Shape: (num_images, channels, height, width)\n",
    "\n",
    "test_images = test_images.to(device)\n",
    "outputs = model(test_images)\n",
    "predicted = (outputs < 0.5).int()\n",
    "\n",
    "print(predicted.shape)  # Shape: (num_images, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv with 2 column ID and label\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"ID\": img_id,\n",
    "    \"Label\": predicted.cpu().numpy().flatten()\n",
    "})\n",
    "df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Submission file created: submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11654041,
     "sourceId": 97835,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
